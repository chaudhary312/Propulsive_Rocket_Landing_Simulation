{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rocket_Landing_Simulation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R2JLzlo6gmJ"
      },
      "source": [
        "# ! apt update\n",
        "# ! apt install -y python3-dev zlib1g-dev libjpeg-dev cmake swig python-pyglet python3-opengl libboost-all-dev libsdl2-dev libosmesa6-dev patchelf ffmpeg xvfb\n",
        "# ! pip install git+https://github.com/openai/gym.git#egg=gym[box2d]\n",
        "# ! pip install xvfbwrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFu_RYfv-v0H"
      },
      "source": [
        "# !pip install tensorflow==1.13.0rc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmlnUFE56sKS"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "\n",
        "from xvfbwrapper import Xvfb\n",
        "vdisplay = Xvfb(width=1280, height=740)\n",
        "vdisplay.start()\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import Math, HTML\n",
        "\n",
        "from pylab import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = 5, 3\n",
        "\n",
        "import gym\n",
        "\n",
        "def render_frames(env, num_frame=50):\n",
        "    env.reset()\n",
        "    frames = []\n",
        "    for i in range(num_frame):\n",
        "        _, _, done, _ = env.step( env.action_space.sample() )\n",
        "        if done:\n",
        "            env.reset()        \n",
        "        frames.append(  env.render(mode=\"rgb_array\") )\n",
        "        \n",
        "    return frames\n",
        "\n",
        "def create_animation(frames):\n",
        "    rc('animation', html='jshtml')\n",
        "    fig = plt.figure()\n",
        "    plt.axis(\"off\")\n",
        "    im = plt.imshow(frames[0], animated=True)\n",
        "\n",
        "    def updatefig(i):\n",
        "        im.set_array(frames[i])\n",
        "        return im,\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, updatefig, frames=len(frames), interval=len(frames)/10, blit=True)\n",
        "    display(HTML(ani.to_html5_video()))    \n",
        "    plt.close()    \n",
        "    \n",
        "    return ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koDFUF3F64TK"
      },
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "import numpy as np\n",
        "import random, tempfile, os\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLwMfsUf7ABc"
      },
      "source": [
        "TRAINING = True # After training change it to 'False' to create visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjhi1o927B-R"
      },
      "source": [
        "LEARNING_RATE = [0.01, 0.001, 0.0001]\n",
        "DISCOUNT_FACTOR = [0.9, 0.99, 0.999]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp5PpGkN7Elg"
      },
      "source": [
        "EPSILON_DECAY = [0.99910, 0.99941, 0.99954, 0.99973, 0.99987]\n",
        "\n",
        "LEARNING_EPISODES = 5000\n",
        "TESTING_EPISODES = 100\n",
        "REPLAY_BUFFER_SIZE = 250000\n",
        "REPLAY_BUFFER_BATCH_SIZE = 32\n",
        "MINIMUM_REWARD = -250\n",
        "STATE_SIZE = 8\n",
        "NUMBER_OF_ACTIONS = 4\n",
        "WEIGHTS_FILENAME = './weights/weights.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95-ZO5527ILy"
      },
      "source": [
        "class Agent:\n",
        "\tdef __init__(self, training, learning_rate, discount_factor, epsilon_decay):\n",
        "\t\tself.training = training\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.discount_factor = discount_factor\n",
        "\t\tself.epsilon_decay = epsilon_decay\n",
        "\t\tself.epsilon = 1.0 if self.training else 0.0\n",
        "\t\tself.replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "\t\tself._create_networks()\n",
        "\n",
        "\t\tself.saver = tf.train.Saver()\n",
        "\n",
        "\t\tself.sess = tf.Session()\n",
        "\t\tself.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\t\tif not training:\n",
        "\t\t\tself._load_weights()\n",
        "\n",
        "\tdef choose_action(self, s):\n",
        "\t\tif not self.training or np.random.rand() > self.epsilon:\n",
        "\t\t\treturn np.argmax(self._Q(np.reshape(s, [1, STATE_SIZE]))[0])\n",
        "\n",
        "\t\treturn np.random.choice(NUMBER_OF_ACTIONS)\n",
        "\n",
        "\tdef store(self, s, a, r, s_, is_terminal):\n",
        "\t\tif self.training:\n",
        "\t\t\tself.replay_buffer.append((np.reshape(s, [1, STATE_SIZE]), a, r, np.reshape(s_, [1, STATE_SIZE]), is_terminal))\n",
        "\n",
        "\tdef optimize(self, s, a, r, s_, is_terminal):\n",
        "\t\tif self.training and len(self.replay_buffer) > REPLAY_BUFFER_BATCH_SIZE:\n",
        "\t\t\tbatch = np.array(random.sample(list(self.replay_buffer), REPLAY_BUFFER_BATCH_SIZE))\n",
        "\t\t\ts = np.vstack(batch[:, 0])\n",
        "\t\t\ta = np.array(batch[:, 1], dtype=int)\n",
        "\t\t\tr = np.array(batch[:, 2], dtype=float)\n",
        "\t\t\ts_ = np.vstack(batch[:, 3])\n",
        "\n",
        "\t\t\tnon_terminal_states = np.where(batch[:, 4] == False)\n",
        "\n",
        "\t\t\tif len(non_terminal_states[0]) > 0:\n",
        "\t\t\t\ta_ = np.argmax(self._Q(s_)[non_terminal_states, :][0], axis=1)\n",
        "\t\t\t\tr[non_terminal_states] += np.multiply(self.discount_factor, self._Q_target(s_)[non_terminal_states, a_][0])\n",
        "\n",
        "\t\t\ty = self._Q(s)\n",
        "\t\t\ty[range(REPLAY_BUFFER_BATCH_SIZE), a] = r\n",
        "\t\t\tself._optimize(s, y)\n",
        "\n",
        "\tdef close(self):\n",
        "\t\tif self.training:\n",
        "\t\t\tprint(\"Saving agent weights to disk...\")\n",
        "\t\t\tsave_path = self.saver.save(self.sess, WEIGHTS_FILENAME)\n",
        "\n",
        "\tdef update(self): \n",
        "\t\tif self.training:\n",
        "\t\t\tQ_W1, Q_W2, Q_W3, Q_b1, Q_b2, Q_b3 = self._get_variables(\"Q\")\n",
        "\t\t\tQ_target_W1, Q_target_W2, Q_target_W3, Q_target_b1, Q_target_b2, Q_target_b3 = self._get_variables(\"Q_target\")\n",
        "\t\t\tself.sess.run([Q_target_W1.assign(Q_W1), Q_target_W2.assign(Q_W2), Q_target_W3.assign(Q_W3), Q_target_b1.assign(Q_b1), Q_target_b2.assign(Q_b2), Q_target_b3.assign(Q_b3)])\n",
        "\n",
        "\t\tif self.epsilon > 0.01:\n",
        "\t\t\tself.epsilon *= self.epsilon_decay\n",
        "\n",
        "\tdef _load_weights(self):\n",
        "\t\tprint(\"Loading agent weights from disk...\")\n",
        "\t\ttry:\n",
        "\t\t\tself.saver.restore(self.sess, WEIGHTS_FILENAME)\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(\"Error loading agent weights from disk.\", e)\n",
        "\n",
        "\tdef _optimize(self, s, y):\n",
        "\t\toptimizer, loss, Q_network = self.sess.run([self.optimizer, self.loss, self.Q_network], {self.Q_X: s, self.Q_y: y})\n",
        "\n",
        "\tdef _Q(self, s):\n",
        "\t\treturn self.sess.run(self.Q_network, {self.Q_X: s})\n",
        "\n",
        "\tdef _Q_target(self, s):\n",
        "\t\treturn self.sess.run(self.Q_target_network, {self.Q_target_X: s})\n",
        "\n",
        "\tdef _create_networks(self):\n",
        "\t\twith tf.variable_scope(\"Q\", reuse=tf.AUTO_REUSE):\n",
        "\t\t\tself.Q_X, self.Q_network = self._create_network()\n",
        "\t\t\tself.Q_y = tf.placeholder(shape=[None, NUMBER_OF_ACTIONS], dtype=tf.float32, name=\"y\")\n",
        "\n",
        "\t\twith tf.name_scope(\"loss\"):\n",
        "\t\t\tself.loss = tf.reduce_mean(tf.squared_difference(self.Q_y, self.Q_network))\n",
        "\n",
        "\t\twith tf.name_scope(\"train\"):\n",
        "\t\t\tself.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "\n",
        "\t\twith tf.variable_scope(\"Q_target\"):\n",
        "\t\t\tself.Q_target_X, self.Q_target_network = self._create_network()\n",
        "\n",
        "\tdef _create_network(self):\n",
        "\t\tX = tf.placeholder(shape=[None, STATE_SIZE], dtype=tf.float32, name=\"X\")\n",
        "\n",
        "\t\tlayer1 = tf.contrib.layers.fully_connected(X, 32, activation_fn=tf.nn.relu)\n",
        "\t\tlayer2 = tf.contrib.layers.fully_connected(layer1, 32, activation_fn=tf.nn.relu)\n",
        "\t\tnetwork = tf.contrib.layers.fully_connected(layer2, NUMBER_OF_ACTIONS, activation_fn=None)\n",
        "\n",
        "\t\treturn X, network\n",
        "\n",
        "\tdef _get_variables(self, scope):\n",
        "\t\twith tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
        "\t\t\tW1 = tf.get_variable(\"fully_connected/weights\")\n",
        "\t\t\tW2 = tf.get_variable(\"fully_connected_1/weights\")\n",
        "\t\t\tW3 = tf.get_variable(\"fully_connected_2/weights\")\n",
        "\t\t\tb1 = tf.get_variable(\"fully_connected/biases\")\n",
        "\t\t\tb2 = tf.get_variable(\"fully_connected_1/biases\")\n",
        "\t\t\tb3 = tf.get_variable(\"fully_connected_2/biases\")\n",
        "\n",
        "\t\treturn W1, W2, W3, b1, b2, b3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvA8Lp447OuH"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\tnp.set_printoptions(precision=2)\n",
        "\n",
        "\tenv = gym.make(\"LunarLander-v2\")\n",
        "\taverage_reward = deque(maxlen=100)\n",
        "\n",
        "\tagent = Agent(TRAINING, LEARNING_RATE[2], DISCOUNT_FACTOR[1], EPSILON_DECAY[1])\n",
        "\n",
        "\tprint(\"Alpha: %.4f Gamma: %.3f Epsilon %.5f\" % (agent.learning_rate, agent.discount_factor, agent.epsilon_decay))\n",
        "\t\n",
        "\tfor episode in range(LEARNING_EPISODES if TRAINING else TESTING_EPISODES):\n",
        "\t\tcurrent_reward = 0\n",
        "\n",
        "\t\ts = env.reset()\n",
        "\n",
        "\t\tfor t in range(1000):\n",
        "\t\t\tif not TRAINING: \n",
        "\t\t\t    create_animation(render_frames(env, 300))\n",
        "\n",
        "\t\t\ta = agent.choose_action(s)\n",
        "\t\t\ts_, r, is_terminal, info = env.step(a)\n",
        "\n",
        "\t\t\tcurrent_reward += r\n",
        "\n",
        "\t\t\tagent.store(s, a, r, s_, is_terminal)\n",
        "\t\t\tagent.optimize(s, a, r, s_, is_terminal)\n",
        "\n",
        "\t\t\ts = s_\n",
        "\n",
        "\t\t\tif is_terminal or current_reward < MINIMUM_REWARD:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tagent.update()\n",
        "\t\taverage_reward.append(current_reward)\n",
        "\t\tprint(\"%i, %.2f, %.2f, %.2f\" % (episode, current_reward, np.average(average_reward), agent.epsilon))\n",
        "    create_animation(render_frames(env, 300))\n",
        "\tenv.close()\n",
        "\tagent.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}